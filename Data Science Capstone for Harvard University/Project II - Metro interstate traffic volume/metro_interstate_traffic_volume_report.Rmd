---
title: "Choose Your Own Project: Interstate Traffic Volume"
subtitle: "edX HarvardX: PH125.9x - Data Science: Capstone"
author: "Maria Eugenia Fonseca"
date: "June 2019"
output:
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
header-includes:
- \usepackage{float}

---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
knitr::opts_chunk$set(fig.align = 'center')

Sys.setlocale("LC_TIME", "English")
```


```{r Loading packages, message=FALSE, warning=FALSE, echo=FALSE, results="hide"}
requiredPackages <- c("tidyverse", "lubridate", "caret", "R.utils", "knitr", "kableExtra", "tictoc")
lapply(requiredPackages, library, character.only = TRUE)
```

\newpage

# Overview 
This project is the second assignment of the 'Data Science: Capstone' course (PH125.9x), offered by edX HarvardX. The aim of this project is to use a publicly available dataset to apply machine learning techniques that go beyond standard linear regression and to clearly communicate the process and insights gained from the analysis.

## Introduction

Traffic volume on a road is defined as the number of vehicles passing the measurement point per unit time. The traffic counts can be used by local councils to identify which routes are used most and to either improve that road or provide an alternative if there is an excessive amount of traffic [[1](https://en.wikipedia.org/wiki/Traffic_count)]. 


## Project Description
The objective of this project is to use machine learning models to predict the traffic volume at an American interstate and understand what features are important to explain the transit. Data transformation and feature engineering are included to improve the predictions and, as various modeling approaches are presented, the best model will be selected based on metrics such as the RMSE.

## Dataset

The present project studies the Metro Interstate Traffic Volume Dataset, available at the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume#) [[2](https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume#)]. This dataset is composed of hourly traffic volumes for westbound Interstate 94 (I-94), including weather and holiday features from 2012 to 2018. 

The I-94 is an east-west Interstate Highway connecting the Great Lakes and northern Great Plains regions of the United States. Its western terminus is in Billings, Montana and its eastern terminus is in Port Huron, Michigan [[3](https://en.wikipedia.org/wiki/Interstate_94)]. The measuring point for the traffic volume is roughly midway between Minneapolis and St Paul, in the state of Minnesota, as shown in the figure below [[4](https://github.com/dreyco676/Anomaly_Detection_A_to_Z/blob/master/Anomaly%20Detection%20A%20to%20Z.pptx)].

```{r echo=FALSE, fig.cap="The measuring point for the traffic volume at I-94", out.width = '75%'}
knitr::include_graphics("station301.png")
```

The dataset is downloaded directly from the UCI Machine Learning Repository. The traffic data is provided by the MN Department of Transportation, while the weather data source is OpenWeatherMap. The dataset includes the following variables:

* Response variable:
    + Traffic volume: numeric hourly traffic volume
* Features:
    + Holiday: categorical US National holidays plus regional holiday
    + Temperature: average temperature in kelvin
    + Rain: amount in mm of rain that occurred in the hour
    + Snow: amount in mm of snow that occurred in the hour
    + Clouds: percentage of cloud cover
    + Weather main: short textual description of the current weather
    + Weather description: longer textual description of the current weather
    + Date time: hour of the data collected in local CST time 

Data exploration and visualization, as well as transformation and feature engineering, will be presented in the next section.

# Methods and Analysis

## Data Engineering

Before further analysis we need to engineer the data to the desired format, cleaning imputation problems, handling duplications, changing some data type to factor and creating new features.

```{r Importing the data, message=FALSE, warning=FALSE, echo=FALSE, error=FALSE}
temp <- tempfile()

download.file("https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz", temp, mode = "wb")
try(gunzip(temp, "Metro_Interstate_Traffic_Volume.csv"))
metro <- read.csv("Metro_Interstate_Traffic_Volume.csv")

rm(temp)
```

The dataset contains `r dim(metro)[1]` hourly registers of traffic volume, weather and holiday features. The first observation is dated `r min(date(metro$date_time))` and the last on `r max(date(metro$date_time))`, but between August 2014 and June 2015 there are no registers.

```{r message=FALSE, warning=FALSE, echo=FALSE}
glimpse(metro)
```

The dataset presents duplicated problems. `r sum(duplicated(metro))` observations are recorded twice, and `r dim(metro)[1] - length(unique(metro$date_time))` observations are duplicated per date time (the only different features are the weather descriptions). We also have some observations described as *thunderstorm* or other rain-related description, but with 0 mm of rain that occurred in the hour. The problem occurred similarly with the snow feature.

```{r message=FALSE, warning=FALSE, echo=FALSE}
metro %>%
  filter(date_time == "2012-10-26 09:00:00") %>%
  select(Holiday = holiday, Temperature = temp, Rain = rain_1h, Snow = snow_1h, Clouds = clouds_all,
         "Weather main" = weather_main, "Weather description" = weather_description,
         "Date time" = date_time, "Traffic volume" = traffic_volume) %>%
  kable() %>%
  kable_styling(latex_options="scale_down", bootstrap_options = "striped", full_width = F)
```

The duplicated observations were removed. The weather description features were kept and we fitted the model with this feature and without. The models without the variable performed slightly better, so in this report, we will only present the analysis without it.

```{r message=FALSE, warning=FALSE, echo=FALSE}
metro2 <- metro %>%
  unique() %>%
  mutate(
    # Fixing the data. 2016-12-26 is not Christmas Day, 2016-12-25 is
    holiday = ifelse(date_time == "2016-12-26 00:00:00", "None",
      ifelse(date_time == "2016-12-25 00:00:00", "Christmas Day", as.character(holiday))
    ),
    date = date(date_time),
    hour = factor(hour(date_time)),
    month = factor(month(date_time)),
    year = factor(year(date_time)),
    weekday = factor(weekdays(date),
      levels = c(
        "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"
      )
    ),
    weather_main = fct_recode(weather_main, "Other" = "Smoke", "Other" = "Squall")
  ) %>%
  group_by(date) %>%
  # If any hour of the day has the classification of a holiday, apply it to the rest of the day
  mutate(
    holiday = ifelse(any(!holiday == "No"), holiday[which(metro$holiday != "No")], "No"),
    is_holiday = ifelse(holiday == "None", "No", "Yes")
  ) %>%
  ungroup() %>%
  filter(
    # Removing strange observations
    temp != 0,               # 0 degrees in Kelvin is not a possible value 
    rain_1h != 9831.3) %>%   # 9831.3 mm of rain is not a possible value. The record is 305 mm/hour
  group_by(date_time) %>%
  # Fixing observations with more than 1 temperature
  mutate(temp = mean(temp),
         rain_1h = mean(rain_1h),
         clouds_all = mean(clouds_all)) %>%
  ungroup() %>%
  mutate(holiday_pre = factor(ifelse(! is_holiday[match(date+1, date)] %in% c("No", NA), "pre holiday", "No")),
         holiday_pos = factor(ifelse(! is_holiday[match(date-1, date)] %in% c("No", NA), "pos holiday", "No"))) %>%
  dplyr::select(- weather_main, -  weather_description) %>%
  unique()
```

New features were created from the original dataset: day, hour, month, year and weekday. The date 2016-12-26 was mislabeled classified as Christmas Day, so we changed it to 2016-12-25. Besides, when analyzing the entire holiday variable, we noticed that a holiday was only labeled as such on its first hour. For instance, 2012-12-25 00:00:00 is labeled *Christmas Day*, but 2012-12-25 01:00:00 and all the other following hours of the mentioned day are labeled as *none*. We corrected this in the dataset and moved from `r sum(metro$holiday != "None")` observations labeled as holidays to `r sum(metro2$holiday != "None")`. We created a binary variable that acknowledges if a day is or not a holiday and two other binary variables that notice if the previous or following day is a holiday. We noticed a few observations with more than one measure of temperature, rain or cloud cover; in these cases, we calculated the mean of the observed values.

Ten observations had a registered temperature of 0 Kelvin, which is not possible as it has never been observed on Earth. In like manner, a rain of 9831.3 mm per hour was observed, when the record registered is a much lower 305 mm/hour. All eleven observations were removed from the dataset.

## Exploratory Data Analysis

Exploratory Data Analysis refers to the critical process of performing initial investigations on data to discover patterns, to spot anomalies, to test hypothesis and to check assumptions with the help of summary statistics and graphical representations [[5](https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15)].

The traffic volume follows a multimodal distribution with three peaks. The first peak contains the highest frequency of traffic value, below 1000 vehicles per hour. The second peak occurs around 3000 vehicles/hour and the third peak, around 4500.

```{r message=FALSE, warning=FALSE, echo=FALSE, out.width='75%'}
metro2 %>%
  ggplot(aes(traffic_volume)) +
  geom_histogram(bins = 35, fill = "steelblue") +
  scale_x_continuous(breaks = seq(0, 7300, by = 1000)) +
  labs(title = "Histogram of traffic volume",
       x = "Traffic volume", y = "Count", fill = element_blank()) +
  theme_classic()
```
  
Some new features were created from the original dataset, such as the weekday. As shown in the boxplot below, the traffic volume appears to increase slowly over the weekdays and is considerably lower on weekends.

```{r message=FALSE, warning=FALSE, echo=FALSE, out.width='75%'}
metro2 %>%
  ggplot(aes(x = weekday, y = traffic_volume)) +
  geom_boxplot(fill = "steelblue", varwidth = T) + 
  labs(
    title = "Boxplot of traffic volume per weekday",
    x = "Weekday", y = "Traffic volume", fill = element_blank()
  ) +
  theme_classic()
```

As presented in the boxplot below, the traffic volume appears to be slightly lower during the holidays.

```{r message=FALSE, warning=FALSE, echo=FALSE, out.width='75%'}
metro2 %>%
  ggplot(aes(y = traffic_volume, x = is_holiday)) +
  geom_boxplot(fill = "steelblue", varwidth = T) + 
  labs(
    title = "Boxplot of traffic volume",
    x = "Holiday", y = "Traffic volume", fill = element_blank()
  ) +
  theme_classic()
```

The traffic volume varies per hour of the day, which is indicative that it will be a good feature of the predictive model. The first big peak in the traffic volume of the day is early in the morning, from 6 to 7 am. The traffic decreases slightly in the late hours of the morning, but increases again after lunch, reaching its maximum between 4 and 5 pm.

```{r message=FALSE, warning=FALSE, echo=FALSE, out.width='75%'}
metro2 %>%
  ggplot(aes(x = hour, y = traffic_volume)) +
  stat_summary(fun.y = mean, colour="steelblue", geom = "line", aes(group = 1), size = 1.5) + 
  labs(
    title = "Traffic volume per hour",
    x = "Hour", y = "Average hourly traffic volume", fill = element_blank()
  ) +
  theme_classic()
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
metro_ml <- metro2 %>%
  select(holiday, temp, rain_1h, clouds_all,
         traffic_volume, hour, weekday, year, holiday_pre, holiday_pos)

metro_train <- metro_ml %>% filter(year != 2018) %>% select(-year) %>% droplevels()
metro_test  <- metro_ml %>% filter(year == 2018) %>% select(-year) %>% droplevels()
```

In machine learning is important to consider two different datasets - the training dataset and the test dataset. The training dataset is the sample of data used to fit an algorithm, and the test dataset is the sample of data used to provide an unbiased evaluation of the final model fit. For this study, our train set will be all the data until 2017 (`r dim(metro_train)[1]` observations) and our test set will be data of the year 2018 (`r dim(metro_test)[1]` observations).

As only `r sum(metro2$snow_1h != 0)` observations had snow registered, and none in the last year (test data) this feature will not be included. To proceed to modeling, the final features considered were holiday, previous or following day of a holiday, temperature, percentage of cloud cover, hour of the day and weekday.


## Modeling Approaches

To guide the process of choosing the machine learning model, we did some experimenting with a subset of the data. We modeled 1-year of data with algorithms such as elastic net, bagged tree and SVM and choose Caret's eXtreme Gradient Boosting (xgbTree) because of a tradeoff between RMSE and execution time. In order not to overstretch this report as some of those models took a long time to run, the experimentation part was omitted and we will focus on tunning the boosting model.

### Linear model and xgbTree model with default hyperparameters

To provide a reference point we set up two baseline models: a simple linear regression model and the xgbTree model with default hyperparameters. This is to see what kind of effect the tuning has on the model performance. All fits were modeled with the same train control, performing cross-validation with 3 folds.

The eXtreme Gradient Boosting (xgbTree) model has seven tuning parameters: number of boosting iterations, maximum tree depth, shrinkage, gamma - minimum loss reduction, subsample ratio of columns, minimum sum of instance weight and subsample percentage. We will change these tuning parameters in steps so that our grid is not too big.

```{r message=FALSE, warning=FALSE, echo=FALSE}
fitControl <- caret::trainControl(
  method = "cv", # cross-validation
  number = 3 # with n folds
)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
predict_and_measure <- function(model, model_name, train_data, test_data, tm) {
  
  train_x <- train_data %>% select(- traffic_volume)
  train_y <- train_data %>% select(traffic_volume)  

  test_x <- test_data %>% select(- traffic_volume)
  test_y <- test_data %>% select(traffic_volume)  

  pred_train <- predict(model, train_x) 
  RMSE_train <- RMSE(obs = train_y , pred = pred_train)
  
  pred_test <- predict(model , test_x) 
  RMSE_test <- RMSE(obs = test_y , pred = pred_test)
  
  perf_grid = data.frame(Predictor = c(model_name),
                         "RMSE (train)" = c(round(RMSE_train, 2)),
                         "RMSE (test)" = c(round(RMSE_test, 2)),
                         "R squared (train)" = round(model$results$Rsquared[as.numeric(rownames(model$bestTune))], 2),
                         "Time(secs)" = round(tm, 2))

  perf_grid
}
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
## Linear model
ptm <- proc.time()  
linearReg <- train(traffic_volume ~ .,
                   data = metro_train, 
                   method = "lm",
                   preProcess = c('center', 'scale'),
                   trControl = fitControl)   
tm <- proc.time() - ptm  
grid <- predict_and_measure(linearReg, 'Linear model', metro_train, metro_test, tm[[3]])

# xgbTree - default hyperparameters
ptm <- proc.time()
xgbTree_default <- train(traffic_volume ~.,
                         data = metro_train,
                         method = "xgbTree",
                         trControl = fitControl)
tm <- proc.time() - ptm
grid <- rbind(grid, predict_and_measure(xgbTree_default, 'xgbTree - Default', metro_train, metro_test, tm[[3]]))

grid %>% 
  select(Predictor, "RMSE (train)" = RMSE..train., "R squared (train)" = R.squared..train.,
         "Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) %>%
  kable_styling(latex_options = "striped", position = "center")
```

The default eXtreme Gradient Boosting already improved substancially the RMSE compared to the linear regression (from `r grid[1, 2]` to  `r grid[2, 2]` on the train set). The default model had the following tuning parameters: nrounds = 150, max_depth = 3, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight = 1 and subsample = 0.5.

To get reasonable running time while testing hyperparameter combinations with *caret* we don't want to go over 1000 in the number of boosting iterations. After tuning the other parameters we will come back.

### xgbTree model Step 1: Number of iterations and the learning Rate

For the first step, we created a grid search with different boosting iterations, shrinkage and max tree depth.

```{r message=FALSE, warning=FALSE, echo=FALSE, out.width='70%'}
### Step 1: Number of Iterations and the Learning Rate
tune_grid <- expand.grid(
  nrounds = seq(from = 100, to = 1000, by = 50),
  eta = c(0.1, 0.2, 0.3, 0.4),
  max_depth = c(2, 3, 4, 5),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

# xgbTree
ptm <- proc.time()
xgbTree_step1 <- train(traffic_volume ~.,
                       data = metro_train,
                       method = "xgbTree",
                       trControl = fitControl,
                       tuneGrid = tune_grid)

tm <- proc.time() - ptm
grid <- rbind(grid, predict_and_measure(xgbTree_step1, 'xgbTree - Step 1', metro_train, metro_test, tm[[3]]))

# helper function for the plots
tuneplot <- function(x, probs = .90) {
  ggplot(x) +
    coord_cartesian(ylim = c(quantile(x$results$RMSE, probs = probs), min(x$results$RMSE))) +
    theme_bw()
}

tuneplot(xgbTree_step1)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
grid %>% 
  select(Predictor, "RMSE (train)" = RMSE..train., "R squared (train)" = R.squared..train.,
         "Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) %>%
  kable_styling(latex_options = "striped", position = "center")
```

The best model found within the grid search had the tuning parameters nrounds = `r xgbTree_step1$bestTune$nrounds`, max_depth = `r xgbTree_step1$bestTune$max_depth` and eta = `r xgbTree_step1$bestTune$eta`. As shown in the graph above, for lower shrinkage the model does not seem stable. This first tuning already improved the RMSE considerably, from `r grid[2, 2]` to  `r grid[3, 2]` (`r paste0(round((grid[3, 2] / grid[2, 2] - 1)*100, 2), "%")`).

### xgbTree model Step 2: Maximum Depth and Minimum Child Weight

Now we will fix the shrinkage to the optimal value found and perform a grid search on the minimum child weight and the maximum tree depth to `r xgbTree_step1$bestTune$max_depth` +- 1 (one above and one below the suggested best tune found in the previous step).

```{r message=FALSE, warning=FALSE, echo=FALSE, out.width='70%'}
### Step 2: Maximum Depth and Minimum Child Weight

tune_grid2 <- expand.grid(
  nrounds = seq(from = 100, to = 1000, by = 50),
  eta = xgbTree_step1$bestTune$eta,
  max_depth = c(xgbTree_step1$bestTune$max_depth - 1, xgbTree_step1$bestTune$max_depth, xgbTree_step1$bestTune$max_depth + 1),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = c(0.1, 0.25, 0.5),
  subsample = 1
)

# xgbTree
ptm <- proc.time()
xgbTree_step2 <- train(traffic_volume ~.,
                       data = metro_train,
                       method = "xgbTree",
                       trControl = fitControl,
                       tuneGrid = tune_grid2)

tm <- proc.time() - ptm
grid <- rbind(grid, predict_and_measure(xgbTree_step2, 'xgbTree - Step 2', metro_train, metro_test, tm[[3]]))

tuneplot(xgbTree_step2)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
grid %>% 
  select(Predictor, "RMSE (train)" = RMSE..train., "R squared (train)" = R.squared..train.,
         "Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) %>%
  kable_styling(latex_options = "striped", position = "center")
```

The best model found within the grid search had the tuning parameters max_depth = `r xgbTree_step2$bestTune$max_depth` and min_child_weight = `r xgbTree_step2$bestTune$min_child_weight`. The RMSE did not change with the different minimum child weight.

### xgbTree model Step 3: Subsample ratio of columns and subsample percentage

In the next step, we fix the minimum child weight to the optimal value found previously, set the maximum tree depth to `r xgbTree_step2$bestTune$max_depth` and do a grid search on the subsample ratio of columns and subsample percentage. 

```{r message=FALSE, warning=FALSE, echo=FALSE, out.width='70%'}
### Step 3: Subsample ratio of columns and subsample percentage

tune_grid3 <- expand.grid(
  nrounds = seq(from = 100, to = 1000, by = 50),
  eta = xgbTree_step1$bestTune$eta,
  max_depth = xgbTree_step2$bestTune$max_depth,
  gamma = 0,
  colsample_bytree = c(0.6, 0.8, 1.0),
  min_child_weight = xgbTree_step2$bestTune$min_child_weight,
  subsample = c(0.5, 0.75, 1.0)
)

# xgbTree
ptm <- proc.time()
xgbTree_step3 <- train(traffic_volume ~.,
                       data = metro_train,
                       method = "xgbTree",
                       trControl = fitControl,
                       tuneGrid = tune_grid3)

tm <- proc.time() - ptm
grid <- rbind(grid, predict_and_measure(xgbTree_step3, 'xgbTree - Step 3', metro_train, metro_test, tm[[3]]))

tuneplot(xgbTree_step3)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
grid %>% 
  select(Predictor, "RMSE (train)" = RMSE..train., "R squared (train)" = R.squared..train.,
         "Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) %>%
  kable_styling(latex_options = "striped", position = "center")
```

The best model found within the grid search had the tuning parameters colsample_bytree = `r xgbTree_step3$bestTune$colsample_bytree` and subsample = `r xgbTree_step1$bestTune$subsample`, the same used in step 2. Because fo this, the RMSE is the same.

### xgbTree model Step 4: Gamma

Now we will fix the colsample_bytree and subsamples tuning parameters and perform a grid search on gamma (minimum loss reduction parameter).

```{r message=FALSE, warning=FALSE, echo=FALSE}
### Step 4: Gamma

tune_grid4 <- expand.grid(
  nrounds = seq(from = 100, to = 1000, by = 50),
  eta = xgbTree_step1$bestTune$eta,
  max_depth = xgbTree_step2$bestTune$max_depth,
  gamma = c(0, 0.05, 0.1, 0.5, 0.7, 0.9, 1.0),
  colsample_bytree = xgbTree_step3$bestTune$colsample_bytree,
  min_child_weight = xgbTree_step2$bestTune$min_child_weight,
  subsample = xgbTree_step3$bestTune$subsample
)

# xgbTree
ptm <- proc.time()
xgbTree_step4 <- train(traffic_volume ~.,
                       data = metro_train,
                       method = "xgbTree",
                       trControl = fitControl,
                       tuneGrid = tune_grid4)

tm <- proc.time() - ptm
grid <- rbind(grid, predict_and_measure(xgbTree_step4, 'xgbTree - Step 4', metro_train, metro_test, tm[[3]]))

grid %>% 
  select(Predictor, "RMSE (train)" = RMSE..train., "R squared (train)" = R.squared..train.,
         "Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) %>%
  kable_styling(latex_options = "striped", position = "center")
```

Different gamma values did not have any effect on the model fit (RMSE), so we continue with the previous value.

### xgbTree model Step 5: Reducing the Learning Rate

Now that we have tunned all hyperparameters parameters, we can go back and try different values for the number of boosting iterations and shrinkage. Before, we tried up until 1000 iterations to save running time, but now the grid search executes up to 10000 iterations.

```{r message=FALSE, warning=FALSE, echo=FALSE, out.width='70%'}
### Step 5: Reducing the Learning Rate

tune_grid5 <- expand.grid(
  nrounds = seq(from = 100, to = 10000, by = 100),
  eta = c(0.01, 0.015, 0.025, 0.05, 0.1),
  max_depth = xgbTree_step3$bestTune$max_depth,
  gamma = xgbTree_step3$bestTune$gamma,
  colsample_bytree = xgbTree_step3$bestTune$colsample_bytree,
  min_child_weight = xgbTree_step3$bestTune$min_child_weight,
  subsample = xgbTree_step3$bestTune$subsample
)

# xgbTree
ptm <- proc.time()
xgbTree_step5 <- train(traffic_volume ~.,
                       data = metro_train,
                       method = "xgbTree",
                       trControl = fitControl,
                       tuneGrid = tune_grid5)

tm <- proc.time() - ptm
grid <- rbind(grid, predict_and_measure(xgbTree_step5, 'xgbTree - Step 5', metro_train, metro_test, tm[[3]]))

tuneplot(xgbTree_step5)
```


```{r message=FALSE, warning=FALSE, echo=FALSE}
grid %>% 
  select(Predictor, "RMSE (train)" = RMSE..train., "R squared (train)" = R.squared..train.,
         "Time (secs)" = Time.secs.) %>%
  kable(booktabs = T) %>%
  kable_styling(latex_options = "striped", position = "center")
```

# Results

The final model had the following tuning parameters: 

```{r message=FALSE, warning=FALSE, echo=FALSE}
### Final model
tune_grid_final <- expand.grid(
  nrounds = xgbTree_step5$bestTune$nrounds,
  eta = xgbTree_step5$bestTune$eta,
  max_depth = xgbTree_step5$bestTune$max_depth,
  gamma = xgbTree_step5$bestTune$gamma,
  colsample_bytree = xgbTree_step5$bestTune$colsample_bytree,
  min_child_weight = xgbTree_step5$bestTune$min_child_weight,
  subsample = xgbTree_step5$bestTune$subsample
)

tune_grid_final %>%
  kable(booktabs = T) %>%
  kable_styling(latex_options = "striped", position = "center") 

# xgbTree - Final model
ptm <- proc.time()
xgbTree_final <- train(traffic_volume ~.,
                        data = metro_train,
                        method = "xgbTree",
                        trControl = fitControl,
                        tuneGrid = tune_grid_final)

tm <- proc.time() - ptm
grid <- rbind(grid, predict_and_measure(xgbTree_final, 'xgbTree - Final model', metro_train, metro_test, tm[[3]]))

```

Now we will evaluate the model fit on the test set. A good model that does not overfit performs similarly on both train and test sets. It is expected for the RMSE to increase and, as shown in the table bellow, the RMSE on the train dataset is `r grid[8,2]` and `r grid[8,3]` on the test set. Even with the inflation, the test RMSE still indicates a good fit and is also better than the train RMSE of the xgbTree model with default hyperparameters.

```{r message=FALSE, warning=FALSE, echo=FALSE}
grid %>%
  filter(Predictor == "xgbTree - Final model") %>%
  select(Predictor, "RMSE (train)" = RMSE..train., "RMSE (test)" = RMSE..test.,
         "R squared (train)" = R.squared..train.) %>%
  kable(booktabs = T) %>%
  kable_styling(latex_options = "striped", position = "center")
```

Besides modeling, it is also important to understand what is relevant to explain the traffic volume. From the graph below, we can see the 20 most important variables. Hour is the most significant feature to explain transit, and the days Sunday and Saturday are also relevant, pointing out that the traffic does differ on weekends.

```{r message=FALSE, warning=FALSE, echo=FALSE}
importance <- varImp(xgbTree_final)
plot(importance, top = 20)
```


# Conclusion

The objective of this project was to use machine learning models to predict the traffic volume at an American interstate and understand what features were important to explain the transit. In this report, we tried many models, created some new features, selected Caret's eXtreme Gradient Boosting (xgbTree) because of a tradeoff between RMSE and execution time and tunned its seven different hyperparameters. The final model obtained an RMSE of `r grid[8,2]` in the training set and `r grid[8,3]` in the test set. The most important feature to explain traffic is *hour*.

# References

https://en.wikipedia.org/wiki/Traffic_count

https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volume#

https://en.wikipedia.org/wiki/Interstate_94

https://github.com/dreyco676/Anomaly_Detection_A_to_Z/blob/master/Anomaly%20Detection%20A%20to%20Z.pptx

https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15

https://www.kaggle.com/pelkoja/visual-xgboost-tuning-with-caret


